{
  "projects": [
    {
      "visible": true,
      "category": "âœ¨ Spark",
      "title": "Data pipeline ETL with PySpark and AWS",
      "description": "Build an end-to-end data pipeline that extracts data from different sources, transforms it using PySpark, and loads it into Amazon S3. Steps: Ingest data from various sources (e.g., CSV, JSON, or relational databases) into an S3 bucket. Use PySpark to perform transformations on the raw data (cleaning, filtering, aggregating). Load the processed data into AWS S3 bucket.",
      "tags": ["AWS", "Spark", "PySpark"],
      "image": {
        "src": "../../images/l1.jpeg",
        "alt": "Quick Setup",
        "linkTo": "https://github.com/SateehTeppala/Data-pipeline-ETL-with-PySpark-and-AWS"
      },
      "links": [
        {
          "type": "github",
          "url": "https://github.com/SateehTeppala/Data-pipeline-ETL-with-PySpark-and-AWS"
        },
        {
          "type": "external",
          "url": "https://medium.com/@sateesh.py/simple-data-pipeline-etl-with-pyspark-and-aws-2aa4d21c0244"
        }
      ]
    },
    {
      "visible": true,
      "category": "ðŸ§° Kafka",
      "title": "How did I create a data pipeline using Kafka and Spark?",
      "description": "Simple Kafka integration with Spark",
      "tags": ["Spark", "kafka"],
      "image": {
        "src": "../../images/l2.png",
        "alt": "Extendable Layout"
      },
      "links": [
        {
          "type": "external",
          "url": "https://medium.com/@sateesh.py/how-did-i-create-a-data-pipeline-using-kafka-and-spark-8248fc33a37e  "
        }
      ]
    }
  ],
  "button": {
    "visible": true,
    "label": "Visit on Github",
    "url": "https://github.com/SateehTeppala"
  }
}
